// AWS-specific Nextflow configuration for running on AWS Batch with S3 workDir
//
// Usage:
//   nextflow run haibol2016/fastqfetcher -profile aws \
//     --aws_region us-east-1 \
//     --aws_job_queue your-job-queue \
//     --aws_s3_bucket your-bucket-name \
//     --input SRR123456 \
//     --outdir s3://your-bucket/results


// Ensure your AWS credentials and IAM roles are properly configured for S3 and Batch access.
// aws.accessKey and aws.secretKey are set in the AWS Batch compute environment or in config file.
// Environment variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
// The default profile in ~/.aws/credentials and/or ~/.aws/config.
// See https://www.nextflow.io/docs/edge/aws.html
params {
    // Allow overrides from CLI
    aws_region               = null
    aws_job_queue            = null
    aws_compute_environment  = null
    aws_job_role             = null
    aws_cli_path             = null          // Path to AWS CLI executable on the machine where you launch Nextflow (not on compute nodes)
    aws_s3_bucket            = null          // e.g. 'my-bucket'
    aws_s3_workdir_prefix    = 'nf-work'     // e.g. 'nf-work/pipeline'
}

profiles {
    aws {
        // Container runtime (Batch uses Docker)
        docker.enabled          = true
        conda.enabled           = false
        singularity.enabled     = false
        podman.enabled          = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        apptainer.enabled       = false

        // Executor and AWS Batch settings
        process.executor        = 'awsbatch'

        // AWS configuration
        aws.region              = params.aws_region ?: System.getenv('AWS_DEFAULT_REGION') ?: 'us-east-1'
        aws.batch.cliPath       = params.aws_cli_path
        aws.batch.jobQueue      = params.aws_job_queue ?: ''
        aws.batch.computeEnvironment = params.aws_compute_environment ?: ''
        aws.batch.jobRole       = params.aws_job_role ?: ''

        // Set work directory on S3 if provided
        // If aws_s3_bucket is set, use S3 for work directory; otherwise use local
        workDir = params.aws_s3_bucket ? "s3://${params.aws_s3_bucket}/${params.aws_s3_workdir_prefix}" : launchDir + '/work'

        // Recommended performance flags for AWS
        aws.client.read_timeout = 10.minutes
        aws.client.max_connections = 100

        // Maximum resource limits per job (adjust based on your Batch compute environment)
        process.resourceLimits = [
            cpus: 64,
            memory: 500.GB,
            time: 7.d
        ]

        // Error handling and retry strategy
        process.errorStrategy = { task.exitStatus in ((130..145) + 104 + 175) ? 'retry' : 'finish' }
        process.maxRetries = 3

        // Optional: enable Wave to materialize environments as containers (if available)
        wave.enabled            = (wave.enabled instanceof Boolean) ? wave.enabled : true
        wave.strategy           = wave.strategy ?: 'conda,container'
    }
}


